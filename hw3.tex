%{
\documentclass[11pt, twoside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex	
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{color}
\usepackage{matlab-prettifier}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{bm}
\usepackage{hyperref} 
\usepackage[section]{placeins}
\lstset{style=Matlab-editor,basicstyle=\ttfamily}

\sloppy
\definecolor{lightgray}{gray}{0.5}
\newenvironment{matlab}{\comment}{\endcomment}
\newenvironment{matlabv}{\lstlisting}{\endlstlisting}	


\title{Homework \# 3 \\ Gaussian Processes}
\author{Cody W. Eilar}
%\date{}							% Activate to display a given date or no date

\begin{document}

\maketitle


\begin{matlab}
%}
close all; 
clear all;

fid = fopen('output_dir/computer.tex', 'w'); 
fprintf(fid, computer); 
fclose(fid); 

fid = fopen('output_dir/matlabver.tex', 'w'); 
a = ver('matlab'); 
fprintf(fid, [a.Name ' version ' a.Version]); 
fclose(fid); 
%{
\end{matlab}

\section{Introduction} 
In the previous homework assignments we focused on processes with support vector machines
and kernels. Support vector machines are great because no \textit{a prior} assumptions
are made about the data. This then allows them to work in a myriad of cases when a
distribution about the data is not known. However, they can become computationally 
infeasible when cross validation is introduced. When dealing with large datasets, it can
take hours or days to properly compute the accuracy of a machine using cross validation 
techniques. This is where Gaussian processes are truly a useful tool. With Gaussian processes
there is no need to cross validate because the uncertainty is built into the model and it 
is therefore safe to assume with some certainty that the data in which you are trying to 
predict or fit will fall into that assumption.

In this paper we will explore how Linear Gaussian Processes work and what their advantages, 
as well as their disadvantages are for regressing and classifying stochastic processes. 
We will also look into extending Gaussian Processes to using \textit{Recursive Kernel Hilbert
Spaces}. We then look at how to optimize the hyper-parameters using exact inference. 
This then will set us up to demonstrate this theory in a few experiments that use
\textbf{MATLAB} to demonstrate a few key principles of Gaussian Processes. 
\section{Theory}
\subsection{Linear Gaussian Process}
We begin the theory section by looking into the signal model and the optimization criterion
for linear processes. As with \textit{SVM}s we are trying to solve Equation \ref{eq:linear_eq}

\begin{align}
f(x) &= \mathbf{x}^T\mathbf{w}\label{eq:linear_eq}
\end{align}

However, in the case of Gaussian Processes, we now assume that there is additive noise 
$\epsilon$ as shown in Equation \ref{eq:add_noise}
\begin{align}
y &= f(x) + \epsilon \label{eq:add_noise}
\end{align}

We take that assumption further and say that the additive noise is independent, 
identically distributed Gaussian noise with zero mean and variance $\sigma^2_n$ \cite{gauss_proc}.
\begin{align}
\epsilon \sim  \mathcal{N}(0, \sigma^2_n) \label{eq:guass_zero}
\end{align}
This now gives rise to what is known as the \textit{likelihood}. That is to say 
given the samples, $X$ and the parameters $\bf{w}$ what is the likelihood
of $\bf{y}$. Mathematically speaking, this is the same as Equation \ref{eq:likelihood}. 
\begin{align}
p(\mathbf{y}|X,\mathbf{w}) = \prod^n_{i=1}p(y_i|\mathbf{x}_i, \mathbf{w})\label{eq:likelihood}
\end{align}
The next step is to find the posterior and the marginal likelihood
\begin{align}
\text{posterior} &= \frac{\text{likelihood} \times \text{prior}}{\text{marginal likelihood}}
\end{align}

And then finally we can write the optimized model as predictive distribution:
\begin{align}
p(f_*|\mathbf{x}_*, X, \mathbf{y}) &= \int p(f_* |\mathbf{x}_*, w)p(\mathbf{w}|X, \mathbf{y})d\mathbf{w} \\
&= \mathcal{N}(\frac{1}{\sigma^2_n} \mathbf{x}_*^TA^{-1} X \mathbf{y}, \mathbf{x}_*^T A^{-1} \mathbf{x}_*)
\label{eq:predictive_dist}
\end{align}


It as at this point where we see that there is no criterion to choose for selecting an optimal model. 
In SVMs for example, we have parameters like cost that we can tweak in order to get a more accurate model. 
Here, however, there is no need for that process because we now have a distribution of where
there data could potentially lie. In Equation \ref{eq:predictive_dist} the $*$ symbol denotes unseen
 or test data and $A = \sigma^{-2}XX^T + \Sigma_p^{-1}$.

\subsection{Gaussian process in Recursive Kernel Hilbert Spaces}
The same process that is used to create the Gaussian predictive distributions can also be used
in kernel spaces. In SVMs, the \textit{kernel trick} allowed us to pass to a higher dimensional 
space easily and still have a tractable the problem. The same is also true for Gaussian processes
\cite{gauss_proc}.In order to achieve this, we create a function $\phi(x)$ which maps a D-dimensional
input vector $x$ into an N dimensional feature space. Thus our new formulation is $f(\mathbf{x}) = 
\phi(\mathbf{x})^T\mathbf{w}$. We can analyze this model the same way that was done in the linear model. 
We start again by showing the predictive distribution as show in \ref{eq:predictive_dist_hilbert}.

\begin{align}
p(\bar{f}_*|\mathbf{x}^*, \mathbf{y}, X) & \sim 
  \mathcal{N}(\mathbf{\bar{w}}^T \mathbf{x}^*, \mathbf{x}^{*T} \mathbf{A}^{-1} \mathbf{x}^*)
\label{eq:predictive_dist_hilbert}
\end{align}

Where $A = \sigma^{-2}XX^T + \Sigma_p^{-1}$ and $\mathbf{\bar{w}} = (\mathbf{XX}^T +
\sigma_n^2 \Sigma_p^{-1})^{-1} \mathbf{Xy}$. Now we can take this formulation and apply
the kernel trick to get the following equations. 

\begin{align}
\mathbf{A} &= \sigma_n^{-2} \mathbf{\Phi \Phi}^T + \Sigma_p^{-1} \\
\mathbf{\bar{w}} &= (\mathbf{\Phi \Phi}^T + \sigma^2_n \Sigma_p^{-1})^{-1} \mathbf{\Phi y}
\end{align}

Where $\Phi$ is the aggregation of columns in $\phi(\mathbf{x})$. Now we can apply the Representer 
Theorem to $\phi(x)$ as $\phi(x) \rightarrow \Sigma^{\frac{1}{2}}_p \phi(x)$ which gives a linear
combination of the transformed data. The estimators can then be rewritten as: 

\begin{align}
\Sigma_p \mathbf{\Phi \alpha} &= (\mathbf{\Phi \Phi}^T + \sigma^2_n \Sigma^{-1}_p)^{-1} \mathbf{\Phi y} \\ 
(\mathbf{\Phi \Phi}^T + \sigma^2_n \Sigma^{-1}_p) \Sigma_p \mathbf{\Phi \alpha} &= \mathbf{\Phi y}
\label{eq:representer_phi}
\end{align}

After multiplying Equation \ref{eq:representer_phi} by $\mathbf{\Phi}^T$ we can obtain the dual 
expression as show in Equation \ref{eq:dual_gauss}.

\begin{align}
\mathbf{\alpha} &= (\mathbf{\Phi}^T \Sigma_p \mathbf{\Phi} + \sigma^2_n \mathbf{I})^{-1} \mathbf{y}\\
&= (\mathbf{K} + \sigma^2_n \mathbf{I})^{-1} \mathbf{y} \label{eq:dual_gauss}
\end{align}

The kernel matrix $k(\cdot, \cdot)$ is known as a covariance function or a kernel. 
The covariance between two random variables is defined as: 

\begin{align}
\text{cov}(f(\mathbf{x}_p), f(\mathbf{x}_q)) = k(\mathbf{x}_p, \mathbf{x}_q) = \text{exp}(-\frac{1}{2}|\mathbf{x}_p - 
\mathbf{x}_q|^2)
\end{align}

If we assume that the process has zero mean, that is to say:

\begin{align}
\mathds{E}[f(\mathbf{x})] = \phi(\mathbf{x})^T \mathds{E}[\mathbf{w}] = 0 \\
\mathds{E}[f(\mathbf{x}) f(\mathbf{x}^{'})] = \phi(\mathbf{x})^T\mathds{E}[\mathbf{ww}^T]\phi(\mathbf{x}^{'}) 
= \phi(\mathbf{x})^T\Sigma_p\phi(\mathbf{x}^{'})
\end{align}

We can show that the kernel function is nothing more than a covariance function because the outputs
are written as a function of the inputs. 


\subsection{Inference over the hyperparameters}
Proper selection of the hyperparameters is key to optimizing the performance of the estimator. One
method to find these parameters is to use cross validation, but in many cases this is not practical when
the dataset is small and the number of hyperparameters is high. But when 
our distribution is uniform, Guassian distribution, we can use a method that is
known as direct inference. With this method, we do not need to use any 
validation techniques to get the optimal parameters, but we can rather get
the hyperparamters directly using the following process: 

\begin{itemize}
  \item Take the gradient of the marginal likelihood
  \item Find the Hessian of the marginal likelihood
  \item Use gradient descent to find all hyperparamters
\end{itemize}

The log marginal likelihood is derived in Chapter 2 of \cite{gauss_proc} and is defined 
as: 
\begin{align}
\text{log}p(\mathbf{y} | X) = -\frac{1}{2} \mathbf{y}^T (K + \sigma^2_n I )^{-1} \mathbf{y} - \frac{1}{2} \text{log}|K + \sigma^2_nI|
- \frac{n}{2} \text{log}(2\pi)
\end{align}

When the distribution is not guaranteed to be Gaussian, the problem can suffer from 
having several local minima or maxima \cite{gauss_proc}, the trick is to find where 
they are. In this case, other choices are more appropriate. 


\section{Methodology and Experiments}
In this section we go do a few various experiments using \textbf{MATLAB} to demonstrate how
Gaussian processes work and what happens when various parameters are modified. We first look
at Linear Gaussian Processes, then provide the experimental setup, observe how well these
processes work for prediction with and without noise, and then finally we take a brief look
at nonlinear Gaussian Processes. 
\subsection{Linear Gaussian Process}
\subsubsection{Experimental Setup}
The experimental for this section involves creating data using an autoregressive-moving average model (ARMA)
. The expression for an ARMA process is show in Equation \ref{eq:arma}. This model is often used for analyzing
time series data. The ARMA model can be used for predicting and comprehending values in a time series $X_t$. 
\begin{align}
X_t = c + \varepsilon_t + \sum_{i=1}^p \varphi_i X_{t-i} + \sum_{i=1}^q \theta_i \varepsilon_{t-i} \label{eq:arma}
\end{align}

The variable $X_t$ refers to the time series data, $c$ is a constant, $\varepsilon_t$ is white noise, $\varphi$ is a
list of  parameters for the auto regressive model (in our case this $\mathbf{a}$), $\theta$ represents the parameters 
for the moving average model (in our case this is $\mathbf{b}$).

For our experimental setup, we use the following coefficients for the moving average coefficients: 
\begin{matlab}
%}
b = [0.0048, 0.0193, 0.0289, 0.0193, 0.0048];
b_str = mat2str(b);
b_str = b_str(2:end-1);
fid = fopen('output_dir/b_str.tex', 'w'); 
fprintf(fid, b_str); 
fclose(fid); 


a =[2.3695, -2.3140, 1.0547, -0.1874];
a_str = mat2str(a);
a_str = a_str(2:end-1);
fid = fopen('output_dir/a_str.tex', 'w'); 
fprintf(fid, a_str); 
fclose(fid); 
%{
\end{matlab}
\begin{align}
\mathbf{b} = \{\input{output_dir/b_str}\}^T
\end{align}

and the following auto-regressive coefficients: 
\begin{align}
\mathbf{a} = \{\input{output_dir/a_str}\}^T
\end{align}

The input into the system consists of 100 samples of Gaussian 
noise with unit variance and zero mean. This data shall be known 
as $\omega[n]$ and a uniformly distributed random subset of these 
data points shall be known as $f[n]$ which will be considered the training 
data.

\begin{matlab}
%}
num_samples = 100;
num_subsamples = 20;
seed = 1; 
rng('default')
rng(seed); 
epsilon = randn(1,num_samples);
plot(epsilon) 
title('Noise per t')
ylabel(sprintf('\\epsilon_t'))
xlabel('t')
saveas(gcf, 'figures/input_noise.eps', 'epsc')

rng(seed); 
f_n = filter(b, a, epsilon);
rng(seed); 
r = randperm(num_samples); % Get random permutations
r = r(1:num_subsamples); % Select the first 20 of those permutations
r = sort(r); % Sort the samples in order of occurence
sub_sample = f_n(r); 
plot(f_n); hold on; 
plot(r, sub_sample); 
title(sprintf('$\\omega[n]$ with %d uniformly distributed subsamples.\n', num_subsamples), ...
   'Interpreter', 'latex')
hold off; 
h = legend('$\omega[n]$', 'subsample');
set(h, 'Interpreter', 'latex');
saveas(gcf, 'figures/omega_subsample.eps', 'epsc')

%{
\end{matlab}
\begin{figure}[h]
\centering
\includegraphics[width=4in]{figures/input_noise.eps}
\caption{Gaussian noise with unit variance and zero mean (input into the system)}
\label{fig:input_noise} 
\end{figure}
\FloatBarrier

\begin{figure}[h]
\centering
\includegraphics[width=4in]{figures/omega_subsample.eps}
\caption{Output of the ARMA process with uniformly distributed subsample}
\label{fig:omega_subsample} 
\end{figure}
\FloatBarrier

\subsubsection{Linear Gaussian Process for Prediction}
With our experimental data setup, we can now begin to look at how to use
Gaussian processes to predict where our datapoints are going. To do this, 
We define our Gaussian input as $x[n] = n$ and the output to be $f[n]$. 
We will use the dual formulation in the code to predict these values.  
\begin{matlab}
%}
%%
xtest = 1:100;
close all
xtrain = 1:20; 
ytrain = f_n(xtrain); 
cov_func = @covSEiso;
sigma = .05; 
gauss_proc(r, sub_sample, f_n, xtest, cov_func, sigma);
saveas(gcf, 'figures/guassian.eps', 'epsc')
%%
%{
\end{matlab}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/guassian}
\caption{Linear Gaussian processes attempting to follow 20 randomly sampled points 
from the original ARMA dataset}
\label{fig:gaussian} 
\end{figure}
\FloatBarrier

From Figure \ref{fig:gaussian} we can see that we are doing a pretty good
job of following the ARMA input with only 20 samples 


\subsubsection{Linear ARMA and AR(1) process noise}
In this section, we create a similar ARMA function as the one used above, but now we are 
going to a noise parameter, $a_n = 0.1$, to the data, $f[n]$.  The output is now: 

\begin{align}
y[n] = f(n) + g[n]
\end{align}

where $g[n] = w_g[n] + 0.2g[n-1]$ and $w_g[n]$ is white noise of variance 0.1. 

Using again the Gaussian linear process, we plot the results in Figure
\ref{fig:gaussian_noise}. 

\begin{matlab}
%}
%%
close all; 
rng(seed); 
[ my_fig, error] = prob3_2();
saveas(my_fig, 'figures/guassian_noise.eps', 'epsc')

fid = fopen('output_dir/gaussian_noise_mse.tex', 'w'); 
fprintf(fid, mat2str(error)); 
fclose(fid); 
%{
\end{matlab}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/guassian_noise}
\caption{Linear Gaussian processes attempting to follow 20 randomly sampled points 
from the original ARMA dataset now with noise added.  }
\label{fig:gaussian_noise} 
\end{figure}
\FloatBarrier

The MSE calculated in Figure is \ref{fig:gaussian_noise}
\input{output_dir/gaussian_noise_mse}. So even with the added noise to the
original signal, we are still able to maintain a fairly low error rate. 
	
\subsection{Nonlinear Gaussian Process}
In this section, we look into the effectrs of using a non-linear gaussian
process to predict our output. First we begin by reproducing our previous
experiments but the input only contains the last 5 values of $x[n]$. This
is the same as removing $f[n]$ from the model completely. The results of
doing this are shown in Figure \ref{fig:gaussian_five} 

\begin{matlab}
%}
%%
close all 
rng(seed)
[my_fig, error ] = prob3_4a();
saveas(my_fig, 'figures/guassian_five.eps', 'epsc')

fid = fopen('output_dir/gaussian_five_mse.tex', 'w'); 
fprintf(fid, mat2str(error)); 
fclose(fid); 
%{
\end{matlab}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/guassian_five}
\caption{Gaussian process which only contains the last 5 values of $x[n]$}
\label{fig:gaussian_five} 
\end{figure}
\FloatBarrier

Because of this, we have lowered our confidence in the estimations by the
regressions process. This is clear in the results in figure
\ref{fig:gaussian_five}, the margin is spread over a very wide interval.
Now though, we want to see if we can improve our results using the sum of
square exponentials plus a noise matrix. We attempt to do this and optimize
with respect to the hyperparameters for the following conditions: 

\begin{itemize}
   \item With and output $f[n]$ corrupted by the white noise plus AR(1)
   noise from the previous section. 
\item and only with white noise. 
\end{itemize}

The first condition is illustrated in \ref{fig:gaussian_noise_five}.

\begin{matlab}
%}
%%
close all 
rng(seed)
[my_fig, error ] = prob3_4b();
saveas(my_fig, 'figures/guassian_noise_five.eps', 'epsc')

fid = fopen('output_dir/gaussian_noise_five_mse.tex', 'w'); 
fprintf(fid, mat2str(error)); 
fclose(fid); 
%{
\end{matlab}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/guassian_noise_five}
\caption{Gaussian process which only contains the last 5 values of $x[n]$ plus AR(1) and white noise}
\label{fig:gaussian_noise_five} 
\end{figure}
\FloatBarrier

The MSE represented in Figure \ref{fig:gaussian_noise_five} is
\input{output_dir/gaussian_noise_five_mse}. And now we want to compute the
second condition that we mentioned above with only the whitenoise. 

\begin{matlab}
%}
%%
close all 
rng(seed)
[my_fig, error ] = prob3_4c();
saveas(my_fig, 'figures/guassian_only_noise_five.eps', 'epsc')

fid = fopen('output_dir/gaussian_only_noise_five_mse.tex', 'w'); 
fprintf(fid, mat2str(error)); 
fclose(fid); 
%{
\end{matlab}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/guassian_only_noise_five}
\caption{Gaussian process which only contains the last 5 values of $x[n]$ and includes \textit{only} white noise}
\label{fig:guassian_only_noise_five} 
\end{figure}
\FloatBarrier

Figure \ref{fig:guassian_only_noise_five} contains only white noise added
to the signal and has an MSE of
\input{output_dir/gaussian_only_noise_five_mse}. This is infact lower than
the previous estimate of the noise. 

\section{Discussion and Conclusion}

\bibliography{hw3.bib}
\bibliographystyle{ieeetr}

\end{document}  
%}

